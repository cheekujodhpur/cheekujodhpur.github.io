<html>
        <head>
                <title>Kumar Ayush</title>
                <link rel = "stylesheet" href = "../main.css" />
                <script src="http://code.jquery.com/jquery-git.js"></script>
                <script src = "../main.js"></script>
        	<script type="text/x-mathjax-config">
		       MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']],
                       displayMath: [['\\[','\\]'], ['$$','$$']]}});
		</script>
		<script type="text/javascript"
  		src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
	</head>
	<body>
		<div id = "pic_y_nome">
                        <a href = "../index.html"><img id = "dp" src = "../images/profile.jpg" /></a>
                	<span class = "nome" id = "nome_title">kumar</span><span class = "nome" id = "nome">AYUSH</span>
		</div>
		<br />
		<br />
		<div style = "width:600px;">
		<h1 style = "color:cadetblue">Validating Cosmological Principle</h1>
		<p>
		In one of MnP group discussions at IITB, Atul Kedia introduced one of his computational projects; determining  the scale at which homogenity and isotropy of the universe is valid based on SDSS DR10. The calculations were based on some concet related to fractal beyond my mathematical skillset. Though, the disucssion insired me to do a smaller version of it during the weekend with Sandesh Kalantre. We could very roughly calculate the densities of galaxies in at different redshifts lying in a spherical shell. That would give us a crude idea of homogenity scale and then we figured to move onto isotroy later.</p>
		<p>
		We collected DR10 redshift data using following CASJOBS query:
		</p>
		<pre>
SELECT SpecObjID,ra,dec,z,zErr,zWarning,class
FROM DR10.SpecObj
INTO mydb.MyTable
WHERE z BETWEEN 0.001 AND 1.0
		</pre>
		<p>
		This was filtered for GALAXY. We kept our range within $z = 1$, with the best reason being that our classical corresondence of CDM model fails. We use the following python code to get frequency of galaxies in redshift bins of width $0.001$, thus amounting to $1000$ bins:
		</p>
		<pre>
import numpy as np
import collections

data = np.loadtxt("data.csv",delimiter=",",skiprows=1,usecols=(1,2,3,4))
bins = collections.defaultdict(lambda:1)
for i in data:
        s = int(i[2]*1000)
        bins[str(s)] += 1

f = open("out.csv","w")

for i in bins.keys():
        f.write(str(i))
        f.write(",")
        f.write(str(float(bins[i])))
        f.write("\n")

f.close()

		</pre>
		<p>
		The plot looks like this:
		</p>
		<img style = "width:500px;" src = "vcp_fig1.png" />
		<p>
		The distinctive peak at about $z=0.01$ is what is known to be Sloan Great Wall, and the observation is expected. What remains unexlained is the peak at about $z=0.55$. One argument was the need to scale the freuqnecy by a factor of square of redshift as we have lesser data for larger redshifts due to observational constraints, but was nullified as there is also a reduction by a factor of square due to a larger number of galaxies at greater redshifts. This comes from our assumtion of homogenity fundamental to the whole discussion.
		</p>
		<p>
		The problem is open for discussion.
		</p>
		</div>
	</body>
</html>
